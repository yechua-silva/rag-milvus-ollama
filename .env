# ----------------------------------------------------
# 🚀 CONFIGURACIÓN DEL SISTEMA RAG
# ----------------------------------------------------
# --- Modo de Ejecución ---
# Cambia a "true" para usar la GPU para generar embeddings (requiere ONNX y drivers compatibles).
# Cambia a "false" para usar la CPU con Ollama (más lento pero más compatible).
USE_GPU="true"

# --- Configuración de Rutas ---
# Carpeta donde se encuentran los documentos PDF a procesar.
DOCS_FOLDER="./docs"

# --- Configuración de Milvus ---
# URI del servidor de Milvus que está corriendo en Docker.
MILVUS_URI="http://127.0.0.1:19530"
# Nombre de la colección donde se guardarán los vectores.
COLLECTION_NAME="pdf_knowledge_base"

# --- Configuración de Modelos ---
# Modelo de embedding para el modo CPU (Ollama). Debe ser un modelo que ya hayas descargado.
EMBEDDING_MODEL="mxbai-embed-large"
# Modelo de embedding para el modo GPU (ONNX). El script lo descargará y convertirá automáticamente.
EMBEDDING_ONNX_MODEL="sentence-transformers/all-MiniLM-L6-v2"
# Modelo de lenguaje (LLM) para generar las respuestas del chat (Ollama).
LLM_MODEL="qwen2.5:3b"

# --- Configuración de Procesamiento de Texto ---
# Tamaño de los chunks de texto en caracteres.
CHUNK_SIZE=800
# Superposición de caracteres entre chunks para no perder contexto.
CHUNK_OVERLAP=100

# --- Configuración de Búsqueda y Rendimiento ---
# Número de chunks relevantes que se recuperarán de Milvus para responder una pregunta.
SEARCH_TOP_K=5
# Tamaño del lote para procesar embeddings en la GPU. Ajusta según la VRAM de tu GPU (32, 64, 128).
EMBEDDING_BATCH_SIZE=64
# Número de procesos paralelos a usar durante la ingesta en modo GPU.
# Se recomienda (Nº de núcleos de tu CPU - 1).
NUM_WORKERS=2