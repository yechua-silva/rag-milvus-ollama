# ----------------------------------------------------
# 游 CONFIGURACI칍N DEL SISTEMA RAG
# ----------------------------------------------------
# --- Modo de Ejecuci칩n ---
# Cambia a "true" para usar la GPU para generar embeddings (requiere ONNX y drivers compatibles).
# Cambia a "false" para usar la CPU con Ollama (m치s lento pero m치s compatible).
USE_GPU="true"

# --- Configuraci칩n de Rutas ---
# Carpeta donde se encuentran los documentos PDF a procesar.
DOCS_FOLDER="./docs"

# --- Configuraci칩n de Milvus ---
# URI del servidor de Milvus que est치 corriendo en Docker.
MILVUS_URI="http://127.0.0.1:19530"
# Nombre de la colecci칩n donde se guardar치n los vectores.
COLLECTION_NAME="pdf_knowledge_base"

# --- Configuraci칩n de Modelos ---
# Modelo de embedding para el modo CPU (Ollama). Debe ser un modelo que ya hayas descargado.
EMBEDDING_MODEL="mxbai-embed-large"
# Modelo de embedding para el modo GPU (ONNX). El script lo descargar치 y convertir치 autom치ticamente.
EMBEDDING_ONNX_MODEL="sentence-transformers/all-MiniLM-L6-v2"
# Modelo de lenguaje (LLM) para generar las respuestas del chat (Ollama).
LLM_MODEL="qwen2.5:3b"

# --- Configuraci칩n de Procesamiento de Texto ---
# Tama침o de los chunks de texto en caracteres.
CHUNK_SIZE=800
# Superposici칩n de caracteres entre chunks para no perder contexto.
CHUNK_OVERLAP=100

# --- Configuraci칩n de B칰squeda y Rendimiento ---
# N칰mero de chunks relevantes que se recuperar치n de Milvus para responder una pregunta.
SEARCH_TOP_K=5
# Tama침o del lote para procesar embeddings en la GPU. Ajusta seg칰n la VRAM de tu GPU (32, 64, 128).
EMBEDDING_BATCH_SIZE=64
# N칰mero de procesos paralelos a usar durante la ingesta en modo GPU.
# Se recomienda (N췈 de n칰cleos de tu CPU - 1).
NUM_WORKERS=2